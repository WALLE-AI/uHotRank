"""
å°† tophub_articles.jsonl æ•°æ®å¯¼å…¥åˆ° Elasticsearch
"""
import json
import logging
from pathlib import Path

from backend.db import ElasticsearchClient, ArticleRepository

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_jsonl(file_path: str) -> list:
    """
    ä» JSONL æ–‡ä»¶åŠ è½½æ•°æ®
    
    Args:
        file_path: JSONL æ–‡ä»¶è·¯å¾„
    
    Returns:
        æ–‡æ¡£åˆ—è¡¨
    """
    documents = []
    
    if not Path(file_path).exists():
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return documents
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                
                try:
                    doc = json.loads(line)
                    documents.append(doc)
                except json.JSONDecodeError as e:
                    logger.warning(f"ç¬¬ {line_num} è¡Œ JSON è§£æå¤±è´¥: {e}")
                    continue
        
        logger.info(f"âœ… ä» {file_path} åŠ è½½äº† {len(documents)} æ¡æ•°æ®")
        return documents
        
    except Exception as e:
        logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return documents


def import_articles_to_es(
    jsonl_file: str = "tophub_articles.jsonl",
    index_name: str = "tophub_articles",
    recreate_index: bool = False
):
    """
    å°†æ–‡ç« æ•°æ®å¯¼å…¥åˆ° Elasticsearch
    
    Args:
        jsonl_file: JSONL æ–‡ä»¶è·¯å¾„
        index_name: ç´¢å¼•åç§°
        recreate_index: æ˜¯å¦é‡æ–°åˆ›å»ºç´¢å¼•
    """
    print("=" * 80)
    print("å¼€å§‹å¯¼å…¥æ•°æ®åˆ° Elasticsearch")
    print("=" * 80)
    
    # 1. åŠ è½½æ•°æ®
    print(f"\nğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®: {jsonl_file}")
    documents = load_jsonl(jsonl_file)
    
    if not documents:
        print("âŒ æ²¡æœ‰æ•°æ®å¯å¯¼å…¥")
        return
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} æ¡æ•°æ®")
    
    # 2. è¿æ¥ Elasticsearch
    print("\nğŸ”Œ æ­£åœ¨è¿æ¥ Elasticsearch...")
    try:
        es_client = ElasticsearchClient()
        print(f"âœ… è¿æ¥æˆåŠŸ")
        
        # æ˜¾ç¤ºé›†ç¾¤ä¿¡æ¯
        info = es_client.get_info()
        print(f"   ç‰ˆæœ¬: {info['version']['number']}")
        print(f"   é›†ç¾¤: {info['cluster_name']}")
        
    except Exception as e:
        print(f"âŒ è¿æ¥å¤±è´¥: {e}")
        print("\nğŸ’¡ è¯·æ£€æŸ¥:")
        print("   1. Elasticsearch æ˜¯å¦å·²å¯åŠ¨")
        print("   2. .env æ–‡ä»¶ä¸­çš„è¿æ¥é…ç½®æ˜¯å¦æ­£ç¡®")
        print("   3. ç¯å¢ƒå˜é‡: ELASTICSEARCH_HOST, ELASTICSEARCH_USERNAME, ELASTICSEARCH_PASSWORD")
        return
    
    # 3. åˆ›å»ºä»“åº“
    print(f"\nğŸ“¦ æ­£åœ¨åˆå§‹åŒ–ç´¢å¼•: {index_name}")
    repo = ArticleRepository(es_client, index_name=index_name)
    
    # 4. åˆ›å»ºç´¢å¼•
    if recreate_index or not repo.index_exists():
        print(f"ğŸ”¨ æ­£åœ¨åˆ›å»ºç´¢å¼•...")
        if repo.create_index(delete_if_exists=recreate_index):
            print(f"âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸ")
        else:
            print(f"âŒ ç´¢å¼•åˆ›å»ºå¤±è´¥")
            return
    else:
        print(f"âœ… ç´¢å¼•å·²å­˜åœ¨")
    
    # 5. æ‰¹é‡å¯¼å…¥æ•°æ®
    print(f"\nğŸ“¥ æ­£åœ¨å¯¼å…¥ {len(documents)} æ¡æ•°æ®...")
    try:
        result = repo.bulk_create_documents(documents)
        
        print("\n" + "=" * 80)
        print("å¯¼å…¥å®Œæˆï¼")
        print("=" * 80)
        print(f"âœ… æˆåŠŸ: {result['success']} æ¡")
        print(f"âŒ å¤±è´¥: {result['failed']} æ¡")
        
        if result['failed'] > 0 and result['failed_items']:
            print("\nå¤±è´¥çš„æ–‡æ¡£:")
            for item in result['failed_items'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  - {item}")
        
        # 6. éªŒè¯å¯¼å…¥
        print(f"\nğŸ” éªŒè¯æ•°æ®...")
        total_count = repo.count()
        print(f"   ç´¢å¼•ä¸­å…±æœ‰ {total_count} æ¡æ–‡æ¡£")
        
        # ç»Ÿè®¡æŠ€æœ¯æ–‡ç« 
        tech_count = repo.count(query={"term": {"tech_detection.is_tech_related": True}})
        print(f"   å…¶ä¸­æŠ€æœ¯ç›¸å…³æ–‡ç« : {tech_count} æ¡")
        
    except Exception as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # 7. å…³é—­è¿æ¥
        es_client.close()
        print("\nâœ… è¿æ¥å·²å…³é—­")


def test_search():
    """æµ‹è¯•æœç´¢åŠŸèƒ½"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•æœç´¢åŠŸèƒ½")
    print("=" * 80)
    
    try:
        es_client = ElasticsearchClient()
        repo = ArticleRepository(es_client)
        
        # 1. å…³é”®è¯æœç´¢
        print("\nğŸ” æµ‹è¯•å…³é”®è¯æœç´¢: 'GPT'")
        results = repo.search_by_keyword("GPT", size=3)
        print(f"æ‰¾åˆ° {len(results)} æ¡ç»“æœ:")
        for i, doc in enumerate(results, 1):
            print(f"\n{i}. {doc.get('title', 'N/A')}")
            print(f"   åˆ†ç±»: {doc.get('category', 'N/A')}")
            print(f"   è¯„åˆ†: {doc.get('_score', 0):.2f}")
        
        # 2. æœç´¢æŠ€æœ¯æ–‡ç« 
        print("\n\nğŸ” æµ‹è¯•æŠ€æœ¯æ–‡ç« æœç´¢ (ç½®ä¿¡åº¦ >= 0.5)")
        tech_articles = repo.search_tech_articles(min_confidence=0.5, size=5)
        print(f"æ‰¾åˆ° {len(tech_articles)} æ¡æŠ€æœ¯æ–‡ç« :")
        for i, doc in enumerate(tech_articles, 1):
            tech_info = doc.get('tech_detection', {})
            print(f"\n{i}. {doc.get('title', 'N/A')}")
            print(f"   åˆ†ç±»: {', '.join(tech_info.get('categories', []))}")
            print(f"   ç½®ä¿¡åº¦: {tech_info.get('confidence', 0)}")
        
        es_client.close()
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="å¯¼å…¥æ–‡ç« æ•°æ®åˆ° Elasticsearch")
    parser.add_argument(
        "--file",
        default="tophub_articles.jsonl",
        help="JSONL æ–‡ä»¶è·¯å¾„ (é»˜è®¤: tophub_articles.jsonl)"
    )
    parser.add_argument(
        "--index",
        default="tophub_articles",
        help="ç´¢å¼•åç§° (é»˜è®¤: tophub_articles)"
    )
    parser.add_argument(
        "--recreate",
        action="store_true",
        help="é‡æ–°åˆ›å»ºç´¢å¼•ï¼ˆä¼šåˆ é™¤å·²æœ‰æ•°æ®ï¼‰"
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="å¯¼å…¥åè¿è¡Œæœç´¢æµ‹è¯•"
    )
    
    args = parser.parse_args()
    
    # å¯¼å…¥æ•°æ®
    import_articles_to_es(
        jsonl_file=args.file,
        index_name=args.index,
        recreate_index=args.recreate
    )
    
    # è¿è¡Œæµ‹è¯•
    if args.test:
        test_search()

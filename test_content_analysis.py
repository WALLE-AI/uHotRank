"""
æµ‹è¯•å†…å®¹åˆ†æåŠŸèƒ½
"""
import asyncio
import logging
from backend.agent.agent_content_keyword_analysis import (
    analyze_article_keywords,
    batch_analyze_articles,
    get_article_statistics
)
from backend.models import LLMProvider
import os

logging.basicConfig(level=logging.INFO)


async def test_single_article():
    """æµ‹è¯•å•ç¯‡æ–‡ç« åˆ†æ"""
    print("=" * 80)
    print("æµ‹è¯•å•ç¯‡æ–‡ç« åˆ†æ")
    print("=" * 80)
    print()
    
    title = "OpenAI å‘å¸ƒ GPT-4 Turboï¼šæ€§èƒ½æå‡ï¼Œæˆæœ¬é™ä½"
    content = """
    OpenAI ä»Šå¤©å®£å¸ƒæ¨å‡º GPT-4 Turboï¼Œè¿™æ˜¯ GPT-4 çš„æœ€æ–°ç‰ˆæœ¬ã€‚
    æ–°ç‰ˆæœ¬åœ¨å¤šä¸ªæ–¹é¢éƒ½æœ‰æ˜¾è‘—æå‡ï¼š
    
    1. ä¸Šä¸‹æ–‡é•¿åº¦ï¼šæ”¯æŒ 128K token çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œç›¸å½“äº 300 é¡µæ–‡æœ¬
    2. æ€§èƒ½æå‡ï¼šæ¨ç†é€Ÿåº¦æå‡ 40%ï¼Œå“åº”æ›´å¿«
    3. æˆæœ¬é™ä½ï¼šAPI ä»·æ ¼ç›¸æ¯”ä¹‹å‰é™ä½ 50%
    4. çŸ¥è¯†æ›´æ–°ï¼šè®­ç»ƒæ•°æ®æ›´æ–°åˆ° 2024 å¹´ 4 æœˆ
    
    GPT-4 Turbo è¿˜å¼•å…¥äº†æ–°çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬ JSON æ¨¡å¼ã€æ”¹è¿›çš„å‡½æ•°è°ƒç”¨ç­‰ã€‚
    å¼€å‘è€…å¯ä»¥é€šè¿‡ OpenAI API è®¿é—®è¿™ä¸ªæ–°æ¨¡å‹ã€‚
    
    ä¸šç•Œä¸“å®¶è®¤ä¸ºï¼Œè¿™æ¬¡æ›´æ–°å°†è¿›ä¸€æ­¥æ¨åŠ¨ AI åº”ç”¨çš„æ™®åŠã€‚
    """
    
    print(f"æ ‡é¢˜: {title}")
    print(f"å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦\n")
    
    print("æ­£åœ¨åˆ†æ...")
    result = await analyze_article_keywords(title, content,provider=LLMProvider.LOCAL)
    
    print("\nåˆ†æç»“æœ:")
    print("-" * 80)
    print(f"âœ… åˆ†ææˆåŠŸ: {result['analysis_success']}")
    print(f"\nğŸ”‘ å…³é”®è¯: {', '.join(result['keywords'])}")
    print(f"\nğŸ“š ä¸»é¢˜: {', '.join(result['topics'])}")
    print(f"\nğŸ“ æ‘˜è¦: {result['summary']}")
    print(f"\nğŸ˜Š æƒ…æ„Ÿ: {result['sentiment']}")
    print(f"\nğŸ“‚ åˆ†ç±»: {result['category']}")
    print(f"\nğŸ·ï¸  å®ä½“:")
    for entity in result['entities']:
        print(f"   - {entity['name']} ({entity['type']})")
    print()


async def test_batch_analysis():
    """æµ‹è¯•æ‰¹é‡åˆ†æ"""
    print("=" * 80)
    print("æµ‹è¯•æ‰¹é‡æ–‡ç« åˆ†æ")
    print("=" * 80)
    print()
    
    articles = [
        {
            "title": "LangChain 0.1.0 æ­£å¼å‘å¸ƒ",
            "content": "æµè¡Œçš„ LLM åº”ç”¨å¼€å‘æ¡†æ¶ LangChain å‘å¸ƒäº† 0.1.0 ç‰ˆæœ¬ã€‚è¿™ä¸ªç‰ˆæœ¬å¼•å…¥äº†å…¨æ–°çš„ Multi-Agent ç³»ç»Ÿï¼Œæ”¯æŒå¤šä¸ª AI Agent ä¹‹é—´çš„åä½œã€‚åŒæ—¶è¿˜æ”¹è¿›äº† RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰çš„æ€§èƒ½ã€‚"
        },
        {
            "title": "Aè‚¡ä»Šæ—¥æ”¶æ¶¨ï¼Œç§‘æŠ€è‚¡é¢†æ¶¨",
            "content": "ä»Šå¤© A è‚¡å¸‚åœºæ•´ä½“è¡¨ç°å¼ºåŠ²ï¼Œä¸Šè¯æŒ‡æ•°æ”¶æ¶¨ 1.5%ã€‚ç§‘æŠ€è‚¡è¡¨ç°å°¤ä¸ºçªå‡ºï¼ŒåŠå¯¼ä½“æ¿å—é¢†æ¶¨ã€‚åˆ†æå¸ˆè®¤ä¸ºï¼Œåœ¨å½“å‰ç»æµç¯å¢ƒä¸‹ï¼Œç§‘æŠ€è‚¡ä»æœ‰ä¸Šæ¶¨ç©ºé—´ã€‚"
        },
        {
            "title": "æ–°ç ”ç©¶ï¼šAI å¯ä»¥é¢„æµ‹è›‹ç™½è´¨ç»“æ„",
            "content": "DeepMind çš„ AlphaFold 3 åœ¨è›‹ç™½è´¨ç»“æ„é¢„æµ‹æ–¹é¢å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚æ–°æ¨¡å‹å¯ä»¥é¢„æµ‹è›‹ç™½è´¨ã€DNAã€RNA ç­‰ç”Ÿç‰©åˆ†å­çš„ç»“æ„ï¼Œå‡†ç¡®ç‡è¾¾åˆ° 95%ã€‚è¿™é¡¹æŠ€æœ¯å°†åŠ é€Ÿæ–°è¯ç ”å‘ã€‚"
        }
    ]
    
    print(f"å¾…åˆ†ææ–‡ç« æ•°: {len(articles)}\n")
    print("æ­£åœ¨æ‰¹é‡åˆ†æ...")
    
    analyzed_articles = await batch_analyze_articles(articles, max_concurrent=2)
    
    print("\nåˆ†æç»“æœ:")
    print("=" * 80)
    
    for i, article in enumerate(analyzed_articles, 1):
        analysis = article.get('content_analysis', {})
        print(f"\n{i}. {article['title']}")
        print(f"   åˆ†ç±»: {analysis.get('category', 'N/A')}")
        print(f"   å…³é”®è¯: {', '.join(analysis.get('keywords', [])[:5])}")
        print(f"   æƒ…æ„Ÿ: {analysis.get('sentiment', 'N/A')}")
        print(f"   æˆåŠŸ: {'âœ…' if analysis.get('analysis_success') else 'âŒ'}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 80)
    print("ç»Ÿè®¡ä¿¡æ¯:")
    print("=" * 80)
    
    stats = get_article_statistics(analyzed_articles)
    
    print(f"\næ€»æ–‡ç« æ•°: {stats['total_articles']}")
    print(f"å·²åˆ†æ: {stats['analyzed_articles']}")
    print(f"åˆ†æç‡: {stats['analysis_rate']}")
    
    print(f"\nçƒ­é—¨å…³é”®è¯:")
    for item in stats['top_keywords'][:10]:
        print(f"  - {item['keyword']}: {item['count']} æ¬¡")
    
    print(f"\nä¸»é¢˜åˆ†å¸ƒ:")
    for item in stats['top_topics'][:5]:
        print(f"  - {item['topic']}: {item['count']} æ¬¡")
    
    print(f"\nåˆ†ç±»ç»Ÿè®¡:")
    for category, count in stats['categories'].items():
        print(f"  - {category}: {count} ç¯‡")
    
    print(f"\næƒ…æ„Ÿç»Ÿè®¡:")
    for sentiment, count in stats['sentiments'].items():
        print(f"  - {sentiment}: {count} ç¯‡")
    print()


async def test_different_content_types():
    """æµ‹è¯•ä¸åŒç±»å‹çš„å†…å®¹"""
    print("=" * 80)
    print("æµ‹è¯•ä¸åŒç±»å‹çš„å†…å®¹")
    print("=" * 80)
    print()
    
    test_cases = [
        {
            "name": "æŠ€æœ¯æ–‡ç« ",
            "title": "vLLM å®ç° LLM æ¨ç†åŠ é€Ÿ",
            "content": "UC Berkeley ç ”ç©¶å›¢é˜Ÿå¼€æºäº† vLLM é¡¹ç›®ï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¼•æ“ã€‚vLLM ä½¿ç”¨ PagedAttention æŠ€æœ¯ï¼Œå¯ä»¥å°†æ¨ç†ååé‡æå‡ 24 å€ã€‚"
        },
        {
            "name": "æ–°é—»æŠ¥é“",
            "title": "æŸå…¬å¸å®£å¸ƒè£å‘˜è®¡åˆ’",
            "content": "æŸç§‘æŠ€å…¬å¸ä»Šå¤©å®£å¸ƒå°†è£å‘˜ 10%ï¼Œæ¶‰åŠçº¦ 1000 åå‘˜å·¥ã€‚å…¬å¸è¡¨ç¤ºè¿™æ˜¯ä¸ºäº†åº”å¯¹ç»æµä¸‹è¡Œå‹åŠ›ã€‚å‘˜å·¥å¯¹æ­¤è¡¨ç¤ºä¸æ»¡å’Œæ‹…å¿§ã€‚"
        },
        {
            "name": "äº§å“è¯„æµ‹",
            "title": "iPhone 15 Pro è¯„æµ‹",
            "content": "iPhone 15 Pro é‡‡ç”¨äº†å…¨æ–°çš„é’›é‡‘å±è¾¹æ¡†ï¼Œé‡é‡æ›´è½»ã€‚A17 Pro èŠ¯ç‰‡æ€§èƒ½å¼ºåŠ²ï¼Œç›¸æœºç³»ç»Ÿä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚æ€»ä½“æ¥è¯´æ˜¯ä¸€æ¬¾ä¼˜ç§€çš„æ——èˆ°æ‰‹æœºã€‚"
        }
    ]
    
    for test_case in test_cases:
        print(f"\næµ‹è¯•: {test_case['name']}")
        print("-" * 80)
        
        result = await analyze_article_keywords(
            test_case['title'],
            test_case['content']
        )
        
        print(f"æ ‡é¢˜: {test_case['title']}")
        print(f"åˆ†ç±»: {result['category']}")
        print(f"æƒ…æ„Ÿ: {result['sentiment']}")
        print(f"å…³é”®è¯: {', '.join(result['keywords'][:5])}")
        print(f"æ‘˜è¦: {result['summary']}")
    
    print()


async def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 80)
    print("å†…å®¹åˆ†æåŠŸèƒ½æµ‹è¯•")
    print("=" * 80)
    print()
    
    print("é€‰æ‹©æµ‹è¯•:")
    print("1. æµ‹è¯•å•ç¯‡æ–‡ç« åˆ†æ")
    print("2. æµ‹è¯•æ‰¹é‡åˆ†æ")
    print("3. æµ‹è¯•ä¸åŒç±»å‹å†…å®¹")
    print("4. è¿è¡Œæ‰€æœ‰æµ‹è¯•")
    
    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1/2/3/4ï¼Œé»˜è®¤ 1): ").strip() or "1"
    print()
    
    if choice == "1":
        await test_single_article()
    elif choice == "2":
        await test_batch_analysis()
    elif choice == "3":
        await test_different_content_types()
    elif choice == "4":
        await test_single_article()
        await test_batch_analysis()
        await test_different_content_types()
    else:
        print("æ— æ•ˆçš„é€‰é¡¹")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

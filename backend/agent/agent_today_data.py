import requests
from bs4 import BeautifulSoup
from newspaper import Article
from fake_useragent import UserAgent  # ç”¨äºéšæœºç”Ÿæˆ User-Agent
import time
import random
import json
import os
from playwright.sync_api import sync_playwright
import logging
import uuid
import asyncio
from curl_cffi import requests as cffi_requests # [ä¿®å¤] æ·»åŠ ç¼ºå¤±çš„å¯¼å…¥

# [ä¿ç•™ä½ çš„åç«¯å¼•ç”¨]
try:
    from backend.utils.url_to_markdown import Crawler, ReadabilityExtractor
    from backend.db import ElasticsearchClient, ArticleRepository
    from backend.agent.agent_content_keyword_analysis import analyze_article_keywords, batch_analyze_articles
except ImportError as e:
    print(f"è­¦å‘Šï¼šåç«¯æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œè¯·ç¡®ä¿ backend ç›®å½•åœ¨è·¯å¾„ä¸­ã€‚é”™è¯¯: {e}")
    # ä¸ºäº†é˜²æ­¢ä»£ç ç›´æ¥å´©æºƒï¼Œè¿™é‡Œå¯ä»¥å®šä¹‰ä¸€äº›å ä½ç±»ï¼Œæˆ–è€…ç›´æ¥æŠ¥é”™åœæ­¢
    pass

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# --- é…ç½®åŒºåŸŸ ---
OUTPUT_FILE = "tophub_articles.jsonl"  # ç»“æœä¿å­˜æ–‡ä»¶ (json lines æ ¼å¼)
MIN_SLEEP = 3  # æœ€çŸ­ç­‰å¾…æ—¶é—´
MAX_SLEEP = 8  # æœ€é•¿ç­‰å¾…æ—¶é—´

# [ä¿®å¤] ä¿®å¤äº†åˆ—è¡¨ç¼ºå°‘é€—å·å’Œä¹±ç çš„é—®é¢˜
category_list = [
    # "å¾®åš",
    # "å¾®ä¿¡",
    "çŸ¥ä¹",
    "è™å—…",
    "ITä¹‹å®¶",
    "æ˜é‡‘",
    "æœºå™¨ä¹‹å¿ƒ",
    "é‡å­ä½",
    # "Readhub",
    # "ç™¾åº¦è´´å§",
    "è™æ‰‘ç¤¾åŒº",
    # "ç¬¬ä¸€è´¢ç»",
    "Product Hunt",
    "å¼€æºä¸­å›½",
    "GitHub",
    "CSDNåšå®¢",
    "UI ä¸­å›½"
]

# å°è¯•åˆå§‹åŒ– crawlerï¼Œå¦‚æœå¯¼å…¥å¤±è´¥åˆ™å¿½ç•¥
try:
    crawler = Crawler()
except NameError:
    crawler = None

# --- æŠ€æœ¯å…³é”®è¯é…ç½® ---
# [ä¿®å¤] ä¿®å¤äº†å­—å…¸é”®å€¼çš„ä¹±ç å’Œå¼•å·
TECH_KEYWORDS = {
    "å¼€æºé¡¹ç›®": [
        "å¼€æº", "open source", "github", "gitlab", "å¼€æºé¡¹ç›®", "å¼€æºåº“",
        "æ–°é¡¹ç›®", "é¡¹ç›®å‘å¸ƒ", "release", "å¼€æºå·¥å…·"
    ],
    "å¤§æ¨¡å‹": [
        "å¤§æ¨¡å‹", "LLM", "GPT", "Claude", "Gemini", "ChatGPT", "è¯­è¨€æ¨¡å‹",
        "å¤§è¯­è¨€æ¨¡å‹", "ç”Ÿæˆå¼AI", "Generative AI", "Foundation Model",
        "Transformer", "BERT", "é¢„è®­ç»ƒæ¨¡å‹"
    ],
    "RAGæŠ€æœ¯": [
        "RAG", "æ£€ç´¢å¢å¼º", "Retrieval Augmented", "å‘é‡æ•°æ®åº“", "Vector Database",
        "Embedding", "çŸ¥è¯†åº“", "æ–‡æ¡£æ£€ç´¢", "è¯­ä¹‰æœç´¢", "Semantic Search"
    ],
    "AgentæŠ€æœ¯": [
        "Agent", "æ™ºèƒ½ä½“", "AI Agent", "è‡ªä¸»ä»£ç†", "Multi-Agent", "å¤šæ™ºèƒ½ä½“",
        "ReAct", "Chain of Thought", "CoT", "Tool Use", "Function Calling"
    ],
    "AIæ¡†æ¶": [
        "LangChain", "LlamaIndex", "AutoGPT", "BabyAGI", "Semantic Kernel",
        "Haystack", "Transformers", "PyTorch", "TensorFlow", "JAX"
    ],
    "æ¨¡å‹è®­ç»ƒ": [
        "å¾®è°ƒ", "Fine-tuning", "RLHF", "LoRA", "QLoRA", "PEFT", "é‡åŒ–",
        "Quantization", "è’¸é¦", "Distillation", "é¢„è®­ç»ƒ", "Pre-training"
    ],
    "æ¨ç†ä¼˜åŒ–": [
        "æ¨ç†åŠ é€Ÿ", "Inference", "vLLM", "TensorRT", "ONNX", "æ¨¡å‹å‹ç¼©",
        "æ¨¡å‹éƒ¨ç½²", "è¾¹ç¼˜è®¡ç®—", "Edge AI"
    ]
}

def get_random_headers():
    """
    éšæœºç”Ÿæˆè¯·æ±‚å¤´ï¼Œä¼ªè£…æˆä¸åŒæµè§ˆå™¨
    """
    try:
        ua = UserAgent()
        user_agent = ua.random
    except:
        # å¦‚æœåº“åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨ UA åˆ—è¡¨
        user_agent_list = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0"
        ]
        user_agent = random.choice(user_agent_list)
        
    return {
        "User-Agent": user_agent,
        "Referer": "https://tophub.today/",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
    }

def save_to_file(data):
    """
    å¢é‡ä¿å­˜ï¼šçˆ¬ä¸€æ¡å­˜ä¸€æ¡ï¼Œé˜²æ­¢ç¨‹åºä¸­æ–­ä¸¢å¤±æ•°æ®
    ä½¿ç”¨ JSONL æ ¼å¼ (æ¯è¡Œä¸€ä¸ª JSON)ï¼Œæ–¹ä¾¿åç»­å¤„ç†
    """
    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:
        f.write(json.dumps(data, ensure_ascii=False) + "\n")
        

def scrape_tophub_dynamic_link():
    with sync_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨ (headless=True è¡¨ç¤ºæ— å¤´æ¨¡å¼ï¼Œä¸æ˜¾ç¤ºç•Œé¢)
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        
        print("æ­£åœ¨åŠ è½½é¡µé¢...")
        page.goto("https://tophub.today")
        
        # ç­‰å¾…ä¸»è¦çš„æ¦œå•å…ƒç´ åŠ è½½å®Œæˆ (æ ¹æ®å®é™… DOM ç»“æ„æ›¿æ¢é€‰æ‹©å™¨)
        try:
            page.wait_for_selector("div.cc-cd", timeout=10000)
        except:
            print("é¡µé¢åŠ è½½è¶…æ—¶æˆ–ç»“æ„å·²å˜æ›´")
            browser.close()
            return []

        # è·å–æ‰€æœ‰æ¦œå•å¡ç‰‡
        nodes = page.query_selector_all("div.cc-cd")
        
        results = []
        
        for node in nodes:
            # è·å–æ¦œå•æ ‡é¢˜
            category_el = node.query_selector("div.cc-cd-lb")
            category = category_el.inner_text().strip() if category_el else "Unknown"
            print(f"å‘ç°æ¦œå•åˆ†ç±»: {category}")
            
            # è·å–æ¦œå•å†…çš„é“¾æ¥
            links = node.query_selector_all("a")
            
            for link in links:
                title_el = link.query_selector("span.t")
                if title_el:
                    title = title_el.inner_text().strip()
                    url = link.get_attribute("href")
                    # å¯¹ç±»åˆ«è¿›è¡Œè¿‡æ»¤
                    if category in category_list:
                        results.append(
                            {"category": category, "title": title, "tophub_url": url}
                        )

        browser.close()
        return results

def get_homepage_links():
    """è·å–é¦–é¡µæ‰€æœ‰æ–‡ç« é“¾æ¥"""
    url = "https://tophub.today"
    print(f"ğŸ“¡ æ­£åœ¨è·å–é¦–é¡µåˆ—è¡¨: {url} ...")
    
    try:
        response = requests.get(url, headers=get_random_headers(), timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æŸ¥æ‰¾æ‰€æœ‰æ¦œå•èŠ‚ç‚¹
        nodes = soup.find_all('div', class_='cc-cd')
        
        all_articles = []
        for node in nodes:
            # è·å–æ¦œå•åˆ†ç±»åç§°
            header = node.find('div', class_='cc-cd-lb')
            category = header.get_text(strip=True) if header else "å…¶ä»–"
            
            # è·å–è¯¥æ¦œå•ä¸‹çš„æ–‡ç« 
            items = node.find_all('a', href=True)
            for item in items:
                title_tag = item.find('span', class_='t')
                title = title_tag.get_text(strip=True) if title_tag else item.get_text(strip=True)
                link = item['href']
                
                # å¤„ç†ç›¸å¯¹é“¾æ¥
                if not link.startswith('http'):
                    link = url + link
                
                # ç®€å•è¿‡æ»¤éæ–‡ç« é“¾æ¥
                if title and "æŸ¥çœ‹æ›´å¤š" not in title:
                    all_articles.append({
                        "category": category,
                        "title": title,
                        "tophub_url": link
                    })
        
        print(f"æˆåŠŸå‘ç° {len(all_articles)} ç¯‡æ–‡ç« é“¾æ¥")
        return all_articles
        
    except Exception as e:
        print(f"è·å–é¦–é¡µå¤±è´¥: {e}")
        return []
    
    
def get_html_stealth(url):
    """
    æ™ºèƒ½è·å–ç½‘é¡µå†…å®¹
    1. ä¼˜å…ˆå°è¯• curl_cffi (é€Ÿåº¦å¿«)
    2. å¦‚æœé‡åˆ° 403/éªŒè¯å¢™ï¼Œè‡ªåŠ¨é™çº§åˆ° Playwright (èƒ½åŠ›å¼º)
    """
    
    # --- ç­–ç•¥ A: é™æ€è¯·æ±‚ (curl_cffi å‡çº§ç‰ˆ) ---
    print(f"   [å°è¯•é™æ€æŠ“å–] {url} ...")
    try:
        # å‡çº§åˆ° chrome124ï¼Œæ¨¡æ‹Ÿæ›´ç°ä»£çš„æµè§ˆå™¨è¡Œä¸º
        response = cffi_requests.get(
            url, 
            impersonate="chrome124", 
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Referer": "https://www.zhihu.com/" # ä¼ªé€ æ¥æº
            },
            timeout=10,
            allow_redirects=True
        )
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯çŸ¥ä¹çš„å®‰å…¨éªŒè¯é¡µé¢ (ç‰¹å¾: åŒ…å« zh-zse-ck æˆ– security.zhihu)
        is_blocked = response.status_code == 403 or "security.zhihu.com" in response.url or "zh-zse-ck" in response.text
        
        if response.status_code == 200 and not is_blocked:
            return response.text
        else:
            print(f"   [é™æ€å¤±è´¥] çŠ¶æ€ç : {response.status_code}ï¼Œè§¦å‘éªŒè¯å¢™ï¼Œå‡†å¤‡åˆ‡æ¢æµè§ˆå™¨...")

    except Exception as e:
        print(f"   [é™æ€æŠ¥é”™] {e}ï¼Œå‡†å¤‡åˆ‡æ¢æµè§ˆå™¨...")

    # --- ç­–ç•¥ B: åŠ¨æ€æµè§ˆå™¨å…œåº• (Playwright) ---
    # ä¸“é—¨å¯¹ä»˜çŸ¥ä¹ã€å¾®ä¿¡ç­‰å¼ºéªŒè¯ç½‘ç«™
    print(f"   [å¯åŠ¨æµè§ˆå™¨å…œåº•] æ­£åœ¨å¯åŠ¨æ— å¤´æµè§ˆå™¨...")
    try:
        with sync_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨ï¼Œheadless=True è¡¨ç¤ºä¸æ˜¾ç¤ºç•Œé¢
            # args å‚æ•°ç”¨äºè§„é¿éƒ¨åˆ†è‡ªåŠ¨åŒ–æ£€æµ‹
            browser = p.chromium.launch(
                headless=True,
                args=['--disable-blink-features=AutomationControlled'] 
            )
            
            # åˆ›å»ºä¸Šä¸‹æ–‡ï¼Œè®¾ç½®è§†çª—å¤§å°ï¼Œä¼ªè£…å¾—æ›´åƒçœŸäºº
            context = browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
            )
            
            page = context.new_page()
            
            # è®¿é—®é¡µé¢
            page.goto(url, wait_until="domcontentloaded", timeout=20000)
            
            # é’ˆå¯¹çŸ¥ä¹ï¼šå¦‚æœé‡åˆ°éªŒè¯ï¼Œç­‰å¾… JS æ‰§è¡Œ
            if "zhihu.com" in url:
                # æ¨¡æ‹Ÿé¼ æ ‡æ»šåŠ¨ï¼Œè§¦å‘åŠ è½½
                page.mouse.wheel(0, 500)
                time.sleep(2) 
                
                # ç­‰å¾…æ ¸å¿ƒå†…å®¹å‡ºç° (QuestionHeader æ˜¯çŸ¥ä¹é—®é¢˜çš„æ ‡å¿—)
                try:
                    page.wait_for_selector("div.QuestionHeader", timeout=5000)
                except:
                    pass # å¦‚æœæ²¡ç­‰åˆ°ä¹Ÿä¸æŠ¥é”™ï¼Œç›´æ¥æ‹¿å½“å‰ HTML
            
            content = page.content()
            browser.close()
            
            print(f"   [æµè§ˆå™¨æˆåŠŸ] è·å–åˆ° {len(content)} å­—èŠ‚")
            return content

    except Exception as e:
        print(f"   [æµè§ˆå™¨ä¹Ÿå¤±è´¥] {e}")
        return None

def gentle_scrape_content(article_info):
    """
    å¯¹å•ç¯‡æ–‡ç« è¿›è¡Œæ¸©å’Œçˆ¬å–
    """
    url = article_info['tophub_url']
    # 1. ä½¿ç”¨æŠ—æ‹¦æˆªæ–¹å¼ä¸‹è½½ HTML
    html = get_html_stealth(url)
    
    if not html:
        return {"title": article_info['title'], "status": "failed_download"}
    
    # æå–æ­£æ–‡å†…å®¹
    try:
        extractor = ReadabilityExtractor()
        extract_content = extractor.extract_article(html)
        extract_content.url = url
        # print("markdown:", extract_content.to_markdown()) # è°ƒè¯•ç”¨
    except NameError:
         # å¦‚æœ ReadabilityExtractor æ²¡æœ‰å¯¼å…¥æˆåŠŸ
        print("Error: ReadabilityExtractor not found")
        return {"title": article_info['title'], "status": "failed", "error": "Missing dependency"}

    try:
        # newspaper3k é…ç½®
        # browser_user_agent å±æ€§éå¸¸é‡è¦ï¼Œnewspaper é»˜è®¤ UA å¾ˆå®¹æ˜“è¢«å°
        article = Article(url, language='zh')
        article.download(input_html=html) # ç›´æ¥ä¼ å…¥å·²ä¸‹è½½çš„HTML
        article.parse()
        
        # ç»„è£…æ•°æ®
        result = {
            "uuid": str(uuid.uuid4()),
            "title": article_info['title'],
            "category": article_info['category'],
            "original_url": article.url, # è·³è½¬åçš„çœŸå®åœ°å€
            "publish_date": str(article.publish_date) if article.publish_date else None,
            "content": extract_content.to_markdown(),
            "images": list(article.images), # è·å–å›¾ç‰‡åˆ—è¡¨
            "scraped_at": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        return result

    except Exception as e:
        # å³ä½¿å¤±è´¥ä¹Ÿè¿”å›åŸºæœ¬ä¿¡æ¯ï¼Œæ ‡è®°é”™è¯¯
        return {
            "title": article_info['title'],
            "category": article_info['category'],
            "error": str(e),
            "status": "failed"
        }


def detect_tech_content(text: str, title: str = "") -> dict:
    """
    æ£€æµ‹æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«æ–°å¼€æºé¡¹ç›®ã€å¤§æ¨¡å‹å‰æ²¿æŠ€æœ¯
    """
    if not text:
        return {
            "is_tech_related": False,
            "categories": [],
            "keywords": [],
            "confidence": 0.0,
            "summary": "å†…å®¹ä¸ºç©º"
        }

    
    # åˆå¹¶æ ‡é¢˜å’Œæ­£æ–‡è¿›è¡Œæ£€æµ‹ï¼ˆæ ‡é¢˜æƒé‡æ›´é«˜ï¼‰
    full_text = (title + " " + title + " " + text).lower()  # æ ‡é¢˜é‡å¤2æ¬¡å¢åŠ æƒé‡
    
    matched_categories = []
    matched_keywords = []
    keyword_count = 0
    
    # éå†æ‰€æœ‰æŠ€æœ¯åˆ†ç±»å’Œå…³é”®è¯
    for category, keywords in TECH_KEYWORDS.items():
        category_matched = False
        for keyword in keywords:
            # ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…
            if keyword.lower() in full_text:
                if keyword not in matched_keywords:
                    matched_keywords.append(keyword)
                    keyword_count += 1
                category_matched = True
        
        if category_matched:
            matched_categories.append(category)
    
    # è®¡ç®—ç½®ä¿¡åº¦
    # åŸºç¡€åˆ†ï¼šåŒ¹é…åˆ°çš„åˆ†ç±»æ•°é‡
    confidence = min(len(matched_categories) * 0.2, 0.6)
    
    # åŠ åˆ†ï¼šåŒ¹é…åˆ°çš„å…³é”®è¯æ•°é‡
    confidence += min(keyword_count * 0.05, 0.3)
    
    # é¢å¤–åŠ åˆ†ï¼šæ ‡é¢˜ä¸­åŒ…å«å…³é”®è¯
    title_lower = title.lower()
    title_match_count = sum(1 for kw in matched_keywords if kw.lower() in title_lower)
    confidence += min(title_match_count * 0.05, 0.1)
    
    # ç¡®ä¿ç½®ä¿¡åº¦åœ¨ 0-1 ä¹‹é—´
    confidence = min(confidence, 1.0)
    
    # åˆ¤æ–­æ˜¯å¦ç›¸å…³ï¼ˆè‡³å°‘åŒ¹é…1ä¸ªåˆ†ç±»ï¼Œä¸”ç½®ä¿¡åº¦ >= 0.2ï¼‰
    is_tech_related = len(matched_categories) > 0 and confidence >= 0.2
    
    # ç”Ÿæˆæ‘˜è¦
    if is_tech_related:
        summary = f"æ£€æµ‹åˆ° {len(matched_categories)} ä¸ªæŠ€æœ¯é¢†åŸŸï¼š{', '.join(matched_categories[:3])}"
        if len(matched_categories) > 3:
            summary += f" ç­‰"
    else:
        summary = "æœªæ£€æµ‹åˆ°ç›¸å…³æŠ€æœ¯å†…å®¹"
    
    return {
        "is_tech_related": is_tech_related,
        "categories": matched_categories,
        "keywords": matched_keywords[:10],  # æœ€å¤šè¿”å›10ä¸ªå…³é”®è¯
        "confidence": round(confidence, 2),
        "summary": summary
    }


def filter_tech_articles(articles: list) -> list:
    """
    ä»æ–‡ç« åˆ—è¡¨ä¸­ç­›é€‰å‡ºæŠ€æœ¯ç›¸å…³çš„æ–‡ç« 
    """
    tech_articles = []
    
    for article in articles:
        title = article.get('title', '')
        content = article.get('content', '')
        
        # è¿›è¡ŒæŠ€æœ¯æ£€æµ‹
        detection_result = detect_tech_content(content, title)
        
        # åªä¿ç•™ç›¸å…³çš„æ–‡ç« 
        if detection_result['is_tech_related']:
            article['tech_detection'] = detection_result
            tech_articles.append(article)
            print(f"å‘ç°æŠ€æœ¯æ–‡ç« : {title}")
            print(f"   åˆ†ç±»: {', '.join(detection_result['categories'])}")
            print(f"   ç½®ä¿¡åº¦: {detection_result['confidence']}")
    
    return tech_articles


def scrape_and_filter_tech_articles(
    save_to_es: bool = True,
    save_to_jsonl: bool = True,
    es_index_name: str = "tophub_articles",
    check_duplicate: bool = True,
    skip_duplicate: bool = True
):
    """
    å®Œæ•´æµç¨‹ï¼šçˆ¬å–æ–‡ç« å¹¶ç­›é€‰æŠ€æœ¯ç›¸å…³å†…å®¹ï¼Œä¿å­˜åˆ° Elasticsearch å’Œ JSONL
    """
    print("=" * 60)
    print("å¼€å§‹çˆ¬å–å¹¶ç­›é€‰æŠ€æœ¯æ–‡ç« ...")
    print("=" * 60)
    
    # åˆå§‹åŒ– ES å®¢æˆ·ç«¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
    es_client = None
    repo = None
    if save_to_es:
        try:
            print("\nğŸ”Œ æ­£åœ¨è¿æ¥ Elasticsearch...")
            es_client = ElasticsearchClient()
            repo = ArticleRepository(es_client, index_name=es_index_name)
            
            # ç¡®ä¿ç´¢å¼•å­˜åœ¨
            if not repo.index_exists():
                print(f"ğŸ“¦ åˆ›å»ºç´¢å¼•: {es_index_name}")
                repo.create_index()
            else:
                print(f"ç´¢å¼•å·²å­˜åœ¨: {es_index_name}")
                
        except Exception as e:
            logger.error(f"è¿æ¥ Elasticsearch å¤±è´¥: {e}")
            print(f"âš ï¸  å°†è·³è¿‡ ES ä¿å­˜ï¼Œä»…ä¿å­˜åˆ° JSONL")
            save_to_es = False
    
    # 1. è·å–é¦–é¡µæ–‡ç« åˆ—è¡¨
    articles = scrape_tophub_dynamic_link()
    if not articles:
        print("æœªè·å–åˆ°æ–‡ç« åˆ—è¡¨")
        if es_client:
            es_client.close()
        return []
    
    print(f"\nğŸ“Š å…±è·å– {len(articles)} ç¯‡æ–‡ç« ï¼Œå¼€å§‹çˆ¬å–å†…å®¹...\n")
    
    # 2. çˆ¬å–æ¯ç¯‡æ–‡ç« çš„è¯¦ç»†å†…å®¹
    detailed_articles = []
    duplicate_count = 0
    
    for i, article_info in enumerate(articles, 1):
        print(f"[{i}/{len(articles)}] æ­£åœ¨çˆ¬å–: {article_info['title']}")
        
        article_content = gentle_scrape_content(article_info)
        
        if article_content.get('status') != 'failed':
            # æ£€æŸ¥é‡å¤
            is_duplicate = False
            if save_to_es and repo and check_duplicate:
                dup_result = repo.check_duplicate(
                    article_content,
                    check_url=True,
                    check_title=True,
                    check_similarity=False  # å¯é€‰ï¼šå¯ç”¨ç›¸ä¼¼åº¦æ£€æµ‹
                )
                
                if dup_result['is_duplicate']:
                    duplicate_count += 1
                    dup_type = dup_result['duplicate_type']
                    logger.info(f"âš ï¸  å‘ç°é‡å¤æ–‡æ¡£ ({dup_type}): {article_content['title']}")
                    
                    if skip_duplicate:
                        print(f"   â­ï¸  è·³è¿‡é‡å¤æ–‡æ¡£ (ç±»å‹: {dup_type})")
                        is_duplicate = True
                    else:
                        print(f"   ğŸ”„ è¦†ç›–é‡å¤æ–‡æ¡£ (ç±»å‹: {dup_type})")
            
            if not is_duplicate:
                detailed_articles.append(article_content)
                
                # å®æ—¶ä¿å­˜åˆ° ESï¼ˆé€æ¡æ’å…¥ï¼‰
                if save_to_es and repo:
                    try:
                        doc_id = article_content.get('original_url') or article_content.get('tophub_url')
                        repo.create_document(article_content, doc_id=doc_id)
                        logger.info(f"å·²ä¿å­˜åˆ° ES: {article_content['title']}")
                    except Exception as e:
                        logger.error(f"ä¿å­˜åˆ° ES å¤±è´¥: {e}")
        
        # ç¤¼è²Œç­‰å¾…
        time.sleep(random.uniform(MIN_SLEEP, MAX_SLEEP))
    
    print(f"\næˆåŠŸçˆ¬å– {len(detailed_articles)} ç¯‡æ–‡ç« ")
    if check_duplicate:
        print(f"â­ï¸  è·³è¿‡ {duplicate_count} ç¯‡é‡å¤æ–‡ç« ")
    
    # 3. ç­›é€‰æŠ€æœ¯ç›¸å…³æ–‡ç« 
    print("\n" + "=" * 60)
    print("å¼€å§‹ç­›é€‰æŠ€æœ¯ç›¸å…³æ–‡ç« ...")
    print("=" * 60 + "\n")
    
    tech_articles = filter_tech_articles(detailed_articles)
    
    print("\n" + "=" * 60)
    print(f"ç­›é€‰å®Œæˆï¼å…±å‘ç° {len(tech_articles)} ç¯‡æŠ€æœ¯ç›¸å…³æ–‡ç« ")
    print("=" * 60)
    
    # 4. ä¿å­˜æŠ€æœ¯æ–‡ç« åˆ°å•ç‹¬çš„æ–‡ä»¶å’Œç´¢å¼•
    if save_to_jsonl and tech_articles:
        tech_output_file = "tech_articles.jsonl"
        with open(tech_output_file, 'w', encoding='utf-8') as f:
            for article in tech_articles:
                f.write(json.dumps(article, ensure_ascii=False) + "\n")
        print(f"\nğŸ’¾ æŠ€æœ¯æ–‡ç« å·²ä¿å­˜åˆ° {tech_output_file}")
    
    # 5. ä¿å­˜æ‰€æœ‰æ–‡ç« åˆ° JSONLï¼ˆå¦‚æœéœ€è¦ï¼‰
    if save_to_jsonl and detailed_articles:
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            for article in detailed_articles:
                f.write(json.dumps(article, ensure_ascii=False) + "\n")
        print(f"ğŸ’¾ æ‰€æœ‰æ–‡ç« å·²ä¿å­˜åˆ° {OUTPUT_FILE}")
    
    # 6. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    if save_to_es and repo:
        try:
            total_count = repo.count()
            tech_count = repo.count(query={"term": {"tech_detection.is_tech_related": True}})
            print(f"\nğŸ“Š Elasticsearch ç»Ÿè®¡:")
            print(f"   ç´¢å¼•: {es_index_name}")
            print(f"   æ€»æ–‡æ¡£æ•°: {total_count}")
            print(f"   æŠ€æœ¯æ–‡ç« æ•°: {tech_count}")
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
    
    # 7. å…³é—­ ES è¿æ¥
    if es_client:
        es_client.close()
        print("\nElasticsearch è¿æ¥å·²å…³é—­")
    
    return tech_articles


def scrape_all_articles_to_es(
    es_index_name: str = "tophub_articles",
    batch_size: int = 10,
    check_duplicate: bool = True,
    skip_duplicate: bool = True,
    enable_analysis: bool = True
):
    """
    çˆ¬å–æ‰€æœ‰æ–‡ç« å¹¶ç›´æ¥ä¿å­˜åˆ° Elasticsearchï¼ˆæ‰¹é‡æ¨¡å¼ï¼‰
    """
    print("=" * 60)
    print("å¼€å§‹çˆ¬å–æ–‡ç« å¹¶ä¿å­˜åˆ° Elasticsearch")
    print("=" * 60)
    
    # 1. è¿æ¥ ES
    try:
        print("\nğŸ”Œ æ­£åœ¨è¿æ¥ Elasticsearch...")
        es_client = ElasticsearchClient()
        repo = ArticleRepository(es_client, index_name=es_index_name)
        
        # ç¡®ä¿ç´¢å¼•å­˜åœ¨
        if not repo.index_exists():
            print(f"ğŸ“¦ åˆ›å»ºç´¢å¼•: {es_index_name}")
            repo.create_index()
        else:
            print(f"ç´¢å¼•å·²å­˜åœ¨: {es_index_name}")
            
    except Exception as e:
        logger.error(f"è¿æ¥ Elasticsearch å¤±è´¥: {e}")
        return {"success": 0, "failed": 0, "duplicate": 0, "analyzed": 0, "error": str(e)}
    
    # 2. è·å–æ–‡ç« åˆ—è¡¨
    articles = scrape_tophub_dynamic_link()
    if not articles:
        print("æœªè·å–åˆ°æ–‡ç« åˆ—è¡¨")
        es_client.close()
        return {"success": 0, "failed": 0, "duplicate": 0, "analyzed": 0, "error": "æœªè·å–åˆ°æ–‡ç« åˆ—è¡¨"}
    
    print(f"\nå…±è·å– {len(articles)} ç¯‡æ–‡ç« ï¼Œå¼€å§‹çˆ¬å–å†…å®¹...\n")
    if check_duplicate:
        print(f"ğŸ” é‡å¤æ£€æµ‹: å·²å¯ç”¨ (è·³è¿‡æ¨¡å¼: {'æ˜¯' if skip_duplicate else 'å¦'})")
    if enable_analysis:
        print(f"ğŸ¤– å†…å®¹åˆ†æ: å·²å¯ç”¨")
    print()
    
    # 3. çˆ¬å–å¹¶æ‰¹é‡ä¿å­˜
    batch = []
    success_count = 0
    failed_count = 0
    duplicate_count = 0
    analyzed_count = 0
    
    # ç”¨äºæ‰¹é‡åˆ†æçš„æ–‡ç« åˆ—è¡¨
    articles_to_analyze = []
    
    for i, article_info in enumerate(articles[:5], 1):
        print(f"[{i}/{len(articles)}] æ­£åœ¨çˆ¬å–: {article_info['title']}")
        
        article_content = gentle_scrape_content(article_info)
        
        if article_content.get('status') != 'failed':
            # æ£€æŸ¥é‡å¤
            is_duplicate = False
            if check_duplicate:
                dup_result = repo.check_duplicate(
                    article_content,
                    check_url=True,
                    check_title=True,
                    check_similarity=False  # å¯é€‰ï¼šå¯ç”¨ç›¸ä¼¼åº¦æ£€æµ‹
                )
                
                if dup_result['is_duplicate']:
                    duplicate_count += 1
                    dup_type = dup_result['duplicate_type']
                    
                    if skip_duplicate:
                        print(f"   â­ï¸  è·³è¿‡é‡å¤æ–‡æ¡£ (ç±»å‹: {dup_type})")
                        is_duplicate = True
                    else:
                        print(f"   ğŸ”„ å°†è¦†ç›–é‡å¤æ–‡æ¡£ (ç±»å‹: {dup_type})")
            
            if not is_duplicate:
                # æ·»åŠ åˆ°å¾…åˆ†æåˆ—è¡¨
                if enable_analysis:
                    articles_to_analyze.append(article_content)
                else:
                    batch.append(article_content)
                
                # è¾¾åˆ°æ‰¹é‡å¤§å°æ—¶ï¼Œè¿›è¡Œåˆ†æå’Œæ’å…¥
                if len(articles_to_analyze) >= batch_size or len(batch) >= batch_size:
                    if enable_analysis and articles_to_analyze:
                        print(f"\n   ğŸ¤– æ­£åœ¨æ‰¹é‡åˆ†æ {len(articles_to_analyze)} ç¯‡æ–‡ç« ...")
                        
                        # ä½¿ç”¨ asyncio è¿è¡Œæ‰¹é‡åˆ†æ
                        try:
                            # ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦å·²æœ‰è¿è¡Œä¸­çš„ loop
                            try:
                                loop = asyncio.get_event_loop()
                            except RuntimeError:
                                loop = asyncio.new_event_loop()
                                asyncio.set_event_loop(loop)
                                
                            analyzed_articles = loop.run_until_complete(
                                batch_analyze_articles(articles_to_analyze, max_concurrent=3)
                            )
                            
                            # ç»Ÿè®¡æˆåŠŸåˆ†æçš„æ•°é‡
                            for article in analyzed_articles:
                                if article.get('content_analysis', {}).get('analysis_success'):
                                    analyzed_count += 1
                            
                            batch.extend(analyzed_articles)
                            articles_to_analyze = []
                            
                            print(f"   åˆ†æå®Œæˆï¼ŒæˆåŠŸ {analyzed_count} ç¯‡")
                            
                        except Exception as e:
                            logger.error(f"   æ‰¹é‡åˆ†æå¤±è´¥: {e}")
                            # å³ä½¿åˆ†æå¤±è´¥ï¼Œä¹Ÿä¿å­˜åŸå§‹æ•°æ®
                            batch.extend(articles_to_analyze)
                            articles_to_analyze = []
                    
                    # æ‰¹é‡æ’å…¥
                    if batch:
                        try:
                            result = repo.bulk_create_documents(batch)
                            success_count += result['success']
                            failed_count += result['failed']
                            print(f"   ğŸ’¾ æ‰¹é‡ä¿å­˜: æˆåŠŸ {result['success']} ç¯‡")
                            batch = []
                        except Exception as e:
                            logger.error(f"æ‰¹é‡ä¿å­˜å¤±è´¥: {e}")
                            failed_count += len(batch)
                            batch = []
        else:
            failed_count += 1
        
        # ç¤¼è²Œç­‰å¾…
        time.sleep(random.uniform(MIN_SLEEP, MAX_SLEEP))
    
    # 4. å¤„ç†å‰©ä½™çš„æ–‡ç« 
    if articles_to_analyze:
        print(f"\nğŸ¤– æ­£åœ¨åˆ†æå‰©ä½™ {len(articles_to_analyze)} ç¯‡æ–‡ç« ...")
        try:
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
            analyzed_articles = loop.run_until_complete(
                batch_analyze_articles(articles_to_analyze, max_concurrent=3)
            )
            
            for article in analyzed_articles:
                if article.get('content_analysis', {}).get('analysis_success'):
                    analyzed_count += 1
            
            batch.extend(analyzed_articles)
            print(f"åˆ†æå®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ‰¹é‡åˆ†æå¤±è´¥: {e}")
            batch.extend(articles_to_analyze)
    
    # ä¿å­˜å‰©ä½™çš„æ–‡ç« 
    if batch:
        try:
            result = repo.bulk_create_documents(batch)
            success_count += result['success']
            failed_count += result['failed']
            print(f"   ğŸ’¾ æ‰¹é‡ä¿å­˜: æˆåŠŸ {result['success']} ç¯‡")
        except Exception as e:
            logger.error(f"æ‰¹é‡ä¿å­˜å¤±è´¥: {e}")
            failed_count += len(batch)
    
    # 5. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 60)
    print("çˆ¬å–å®Œæˆ")
    print("=" * 60)
    print(f"æˆåŠŸ: {success_count} ç¯‡")
    print(f"å¤±è´¥: {failed_count} ç¯‡")
    if check_duplicate:
        print(f"â­ï¸  é‡å¤: {duplicate_count} ç¯‡")
    if enable_analysis:
        print(f"ğŸ¤– å·²åˆ†æ: {analyzed_count} ç¯‡")
    
    try:
        total_count = repo.count()
        tech_count = repo.count(query={"term": {"tech_detection.is_tech_related": True}})
        analyzed_in_es = repo.count(query={"term": {"content_analysis.analysis_success": True}})
        
        print(f"\nğŸ“Š Elasticsearch ç»Ÿè®¡:")
        print(f"   ç´¢å¼•: {es_index_name}")
        print(f"   æ€»æ–‡æ¡£æ•°: {total_count}")
        print(f"   æŠ€æœ¯æ–‡ç« æ•°: {tech_count}")
        if enable_analysis:
            print(f"   å·²åˆ†ææ–‡ç« : {analyzed_in_es}")
            
            # æ˜¾ç¤ºçƒ­é—¨å…³é”®è¯
            print(f"\nğŸ”‘ çƒ­é—¨å…³é”®è¯ (Top 10):")
            top_keywords = repo.get_keyword_statistics(top_n=10)
            for i, item in enumerate(top_keywords[:10], 1):
                print(f"   {i}. {item['keyword']}: {item['count']} æ¬¡")
            
            # æ˜¾ç¤ºçƒ­é—¨ä¸»é¢˜
            print(f"\nğŸ“š çƒ­é—¨ä¸»é¢˜ (Top 5):")
            top_topics = repo.get_topic_statistics(top_n=5)
            for i, item in enumerate(top_topics[:5], 1):
                print(f"   {i}. {item['topic']}: {item['count']} æ¬¡")
            
            # æ˜¾ç¤ºåˆ†ç±»ç»Ÿè®¡
            print(f"\nğŸ“‚ åˆ†ç±»ç»Ÿè®¡:")
            categories = repo.get_category_statistics()
            for category, count in list(categories.items())[:5]:
                print(f"   {category}: {count} ç¯‡")
            
            # æ˜¾ç¤ºæƒ…æ„Ÿç»Ÿè®¡
            print(f"\nğŸ˜Š æƒ…æ„Ÿç»Ÿè®¡:")
            sentiments = repo.get_sentiment_statistics()
            for sentiment, count in sentiments.items():
                print(f"   {sentiment}: {count} ç¯‡")
                
    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
    
    # 6. å…³é—­è¿æ¥
    es_client.close()
    print("\nElasticsearch è¿æ¥å·²å…³é—­")
    
    return {
        "success": success_count,
        "failed": failed_count,
        "duplicate": duplicate_count,
        "analyzed": analyzed_count,
        "total": success_count + failed_count + duplicate_count
    }

if __name__ == "__main__":
    # åœ¨è¿™é‡Œè¿è¡Œï¼Œä¾‹å¦‚ï¼š
    # scrape_and_filter_tech_articles(save_to_es=False) # ä»…æµ‹è¯•çˆ¬å–
    pass
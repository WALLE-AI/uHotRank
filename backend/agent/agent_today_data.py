import requests
from bs4 import BeautifulSoup
from newspaper import Article
from fake_useragent import UserAgent  # ç”¨äºéšæœºç”Ÿæˆ User-Agent
import time
import random
import json
import os
from playwright.sync_api import sync_playwright

# --- é…ç½®åŒºåŸŸ ---
OUTPUT_FILE = "tophub_articles.jsonl"  # ç»“æœä¿å­˜æ–‡ä»¶ (json lines æ ¼å¼)
MIN_SLEEP = 3  # æœ€çŸ­ç­‰å¾…æ—¶é—´ (ç§’)
MAX_SLEEP = 8  # æœ€é•¿ç­‰å¾…æ—¶é—´ (ç§’)

def get_random_headers():
    """
    éšæœºç”Ÿæˆè¯·æ±‚å¤´ï¼Œä¼ªè£…æˆä¸åŒæµè§ˆå™¨
    """
    try:
        ua = UserAgent()
        user_agent = ua.random
    except:
        # å¦‚æœåº“åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨çš„ UA åˆ—è¡¨
        user_agent_list = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0"
        ]
        user_agent = random.choice(user_agent_list)
        
    return {
        "User-Agent": user_agent,
        "Referer": "https://tophub.today/",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
    }

def save_to_file(data):
    """
    å¢é‡ä¿å­˜ï¼šçˆ¬ä¸€æ¡å­˜ä¸€æ¡ï¼Œé˜²æ­¢ç¨‹åºä¸­æ–­ä¸¢å¤±æ•°æ®
    ä½¿ç”¨ JSONL æ ¼å¼ (æ¯è¡Œä¸€ä¸ª JSON)ï¼Œæ–¹ä¾¿åç»­å¤„ç†
    """
    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:
        f.write(json.dumps(data, ensure_ascii=False) + "\n")
        

def scrape_tophub_dynamic_link():
    with sync_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨ (headless=True è¡¨ç¤ºæ— å¤´æ¨¡å¼ï¼Œä¸æ˜¾ç¤ºç•Œé¢)
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        
        print("æ­£åœ¨åŠ è½½é¡µé¢...")
        page.goto("https://tophub.today")
        
        # ç­‰å¾…ä¸»è¦çš„æ¦œå•å…ƒç´ åŠ è½½å®Œæˆ (æ ¹æ®å®é™… DOM ç»“æ„æ›¿æ¢é€‰æ‹©å™¨)
        # ä¾‹å¦‚ç­‰å¾…åŒ…å« 'çƒ­é—¨' å­—æ ·çš„å…ƒç´ æˆ–ç‰¹å®šçš„å¡ç‰‡ class
        try:
            page.wait_for_selector("div.cc-cd", timeout=10000)
        except:
            print("é¡µé¢åŠ è½½è¶…æ—¶æˆ–ç»“æ„å·²å˜")
            browser.close()
            return

        # è·å–æ‰€æœ‰æ¦œå•å¡ç‰‡
        nodes = page.query_selector_all("div.cc-cd")
        
        results = []
        
        for node in nodes[-2:]:
            # è·å–æ¦œå•æ ‡é¢˜
            category_el = node.query_selector("div.cc-cd-lb")
            category = category_el.inner_text().strip() if category_el else "Unknown"
            
            # è·å–æ¦œå•å†…çš„é“¾æ¥
            links = node.query_selector_all("a")
            
            for link in links:
                title_el = link.query_selector("span.t")
                if title_el:
                    title = title_el.inner_text().strip()
                    url = link.get_attribute("href")
                    results.append(
                        {"category": category, "title": title, "tophub_url": url}
                        )

        browser.close()
        return results

def get_homepage_links():
    """è·å–é¦–é¡µæ‰€æœ‰æ–‡ç« é“¾æ¥"""
    url = "https://tophub.today"
    print(f"ğŸ“¡ æ­£åœ¨è·å–é¦–é¡µåˆ—è¡¨: {url} ...")
    
    try:
        response = requests.get(url, headers=get_random_headers(), timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æŸ¥æ‰¾æ‰€æœ‰æ¦œå•èŠ‚ç‚¹
        nodes = soup.find_all('div', class_='cc-cd')
        
        all_articles = []
        for node in nodes:
            # è·å–æ¦œå•åˆ†ç±»åç§°
            header = node.find('div', class_='cc-cd-lb')
            category = header.get_text(strip=True) if header else "å…¶ä»–"
            
            # è·å–è¯¥æ¦œå•ä¸‹çš„æ–‡ç« 
            items = node.find_all('a', href=True)
            for item in items:
                title_tag = item.find('span', class_='t')
                title = title_tag.get_text(strip=True) if title_tag else item.get_text(strip=True)
                link = item['href']
                
                # å¤„ç†ç›¸å¯¹é“¾æ¥
                if not link.startswith('http'):
                    link = url + link
                
                # ç®€å•è¿‡æ»¤éæ–‡ç« é“¾æ¥
                if title and "æŸ¥çœ‹æ›´å¤š" not in title:
                    all_articles.append({
                        "category": category,
                        "title": title,
                        "tophub_url": link
                    })
        
        print(f"âœ… æˆåŠŸå‘ç° {len(all_articles)} ç¯‡æ–‡ç« é“¾æ¥ã€‚")
        return all_articles
        
    except Exception as e:
        print(f"âŒ è·å–é¦–é¡µå¤±è´¥: {e}")
        return []

def gentle_scrape_content(article_info):
    """
    å¯¹å•ç¯‡æ–‡ç« è¿›è¡Œæ¸©å’Œçˆ¬å–
    """
    url = article_info['tophub_url']
    
    try:
        # newspaper3k é…ç½®
        # browser_user_agent å±æ€§éå¸¸é‡è¦ï¼Œnewspaper é»˜è®¤ UA å¾ˆå®¹æ˜“è¢«å°
        article = Article(url, language='zh', browser_user_agent=get_random_headers()['User-Agent'])
        
        article.download()
        article.parse()
        
        # ç»„è£…æ•°æ®
        result = {
            "title": article_info['title'],
            "category": article_info['category'],
            "original_url": article.url, # è·³è½¬åçš„çœŸå®åœ°å€
            "publish_date": str(article.publish_date) if article.publish_date else None,
            "content": article.text,
            "images": list(article.images), # è·å–å›¾ç‰‡åˆ—è¡¨
            "scraped_at": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        return result

    except Exception as e:
        # å³ä½¿å¤±è´¥ä¹Ÿè¿”å›åŸºæœ¬ä¿¡æ¯ï¼Œæ ‡è®°é”™è¯¯
        return {
            "title": article_info['title'],
            "category": article_info['category'],
            "error": str(e),
            "status": "failed"
        }


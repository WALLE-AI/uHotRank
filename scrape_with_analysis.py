"""
å¸¦å†…å®¹åˆ†æçš„çˆ¬è™«è„šæœ¬
"""
import asyncio
import logging
from backend.agent.agent_today_data import (
    scrape_tophub_dynamic_link,
    gentle_scrape_content,
    MIN_SLEEP,
    MAX_SLEEP
)
from backend.agent.agent_content_keyword_analysis import analyze_article_keywords
from backend.db import ElasticsearchClient, ArticleRepository
import time
import random

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def scrape_with_content_analysis(
    es_index_name: str = "tophub_articles",
    enable_analysis: bool = True,
    check_duplicate: bool = True,
    skip_duplicate: bool = True
):
    """
    çˆ¬å–æ–‡ç« å¹¶è¿›è¡Œå†…å®¹åˆ†æï¼Œä¿å­˜åˆ° Elasticsearch
    
    Args:
        es_index_name: Elasticsearch ç´¢å¼•åç§°
        enable_analysis: æ˜¯å¦å¯ç”¨å†…å®¹åˆ†æ
        check_duplicate: æ˜¯å¦æ£€æŸ¥é‡å¤
        skip_duplicate: æ˜¯å¦è·³è¿‡é‡å¤
    
    Returns:
        dict: ç»Ÿè®¡ä¿¡æ¯
    """
    print("=" * 80)
    print("å¼€å§‹çˆ¬å–æ–‡ç« å¹¶è¿›è¡Œå†…å®¹åˆ†æ")
    print("=" * 80)
    
    # 1. è¿æ¥ ES
    try:
        print("\nğŸ”Œ æ­£åœ¨è¿æ¥ Elasticsearch...")
        es_client = ElasticsearchClient()
        repo = ArticleRepository(es_client, index_name=es_index_name)
        
        # ç¡®ä¿ç´¢å¼•å­˜åœ¨
        if not repo.index_exists():
            print(f"ğŸ“¦ åˆ›å»ºç´¢å¼•: {es_index_name}")
            repo.create_index()
        else:
            print(f"âœ… ç´¢å¼•å·²å­˜åœ¨: {es_index_name}")
            
    except Exception as e:
        logger.error(f"âŒ è¿æ¥ Elasticsearch å¤±è´¥: {e}")
        return {"success": 0, "failed": 0, "duplicate": 0, "analyzed": 0, "error": str(e)}
    
    # 2. è·å–æ–‡ç« åˆ—è¡¨
    articles = scrape_tophub_dynamic_link()
    if not articles:
        print("âŒ æœªè·å–åˆ°æ–‡ç« åˆ—è¡¨")
        es_client.close()
        return {"success": 0, "failed": 0, "duplicate": 0, "analyzed": 0, "error": "æœªè·å–åˆ°æ–‡ç« åˆ—è¡¨"}
    
    print(f"\nğŸ“Š å…±è·å– {len(articles)} ç¯‡æ–‡ç« ï¼Œå¼€å§‹çˆ¬å–å†…å®¹...\n")
    if enable_analysis:
        print(f"ğŸ¤– å†…å®¹åˆ†æ: å·²å¯ç”¨\n")
    if check_duplicate:
        print(f"ğŸ” é‡å¤æ£€æµ‹: å·²å¯ç”¨ (è·³è¿‡æ¨¡å¼: {'æ˜¯' if skip_duplicate else 'å¦'})\n")
    
    # 3. çˆ¬å–å¹¶åˆ†æ
    success_count = 0
    failed_count = 0
    duplicate_count = 0
    analyzed_count = 0
    
    for i, article_info in enumerate(articles, 1):
        print(f"[{i}/{len(articles)}] æ­£åœ¨çˆ¬å–: {article_info['title']}")
        
        # çˆ¬å–å†…å®¹
        article_content = gentle_scrape_content(article_info)
        
        if article_content.get('status') == 'failed':
            failed_count += 1
            time.sleep(random.uniform(MIN_SLEEP, MAX_SLEEP))
            continue
        
        # æ£€æŸ¥é‡å¤
        is_duplicate = False
        if check_duplicate:
            dup_result = repo.check_duplicate(
                article_content,
                check_url=True,
                check_title=True,
                check_similarity=False
            )
            
            if dup_result['is_duplicate']:
                duplicate_count += 1
                dup_type = dup_result['duplicate_type']
                
                if skip_duplicate:
                    print(f"   â­ï¸  è·³è¿‡é‡å¤æ–‡æ¡£ (ç±»å‹: {dup_type})")
                    is_duplicate = True
                else:
                    print(f"   ğŸ”„ å°†è¦†ç›–é‡å¤æ–‡æ¡£ (ç±»å‹: {dup_type})")
        
        if not is_duplicate:
            # è¿›è¡Œå†…å®¹åˆ†æ
            if enable_analysis:
                try:
                    print(f"   ğŸ¤– æ­£åœ¨åˆ†æå†…å®¹...")
                    analysis_result = await analyze_article_keywords(
                        title=article_content.get('title', ''),
                        content=article_content.get('content', '')
                    )
                    
                    article_content['content_analysis'] = analysis_result
                    
                    if analysis_result.get('analysis_success'):
                        analyzed_count += 1
                        keywords = analysis_result.get('keywords', [])
                        category = analysis_result.get('category', '')
                        print(f"   âœ… åˆ†æå®Œæˆ: {category} | å…³é”®è¯: {', '.join(keywords[:3])}")
                    else:
                        print(f"   âš ï¸  åˆ†æå¤±è´¥")
                        
                except Exception as e:
                    logger.error(f"   âŒ å†…å®¹åˆ†æå¤±è´¥: {e}")
                    article_content['content_analysis'] = {
                        "keywords": [],
                        "topics": [],
                        "summary": "",
                        "sentiment": "neutral",
                        "category": "æœªåˆ†ç±»",
                        "entities": [],
                        "analysis_success": False
                    }
            
            # ä¿å­˜åˆ° ES
            try:
                doc_id = article_content.get('original_url') or article_content.get('tophub_url')
                repo.create_document(article_content, doc_id=doc_id)
                success_count += 1
                print(f"   ğŸ’¾ å·²ä¿å­˜åˆ° ES")
            except Exception as e:
                logger.error(f"   âŒ ä¿å­˜å¤±è´¥: {e}")
                failed_count += 1
        
        # ç¤¼è²Œç­‰å¾…
        time.sleep(random.uniform(MIN_SLEEP, MAX_SLEEP))
    
    # 4. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 80)
    print("çˆ¬å–å®Œæˆï¼")
    print("=" * 80)
    print(f"âœ… æˆåŠŸ: {success_count} æ¡")
    print(f"âŒ å¤±è´¥: {failed_count} æ¡")
    if check_duplicate:
        print(f"â­ï¸  é‡å¤: {duplicate_count} æ¡")
    if enable_analysis:
        print(f"ğŸ¤– å·²åˆ†æ: {analyzed_count} æ¡")
    
    # 5. æ˜¾ç¤º ES ç»Ÿè®¡
    try:
        total_count = repo.count()
        analyzed_in_es = repo.count(query={"term": {"content_analysis.analysis_success": True}})
        
        print(f"\nğŸ“Š Elasticsearch ç»Ÿè®¡:")
        print(f"   ç´¢å¼•: {es_index_name}")
        print(f"   æ€»æ–‡æ¡£æ•°: {total_count}")
        print(f"   å·²åˆ†ææ–‡æ¡£: {analyzed_in_es}")
        
        # æ˜¾ç¤ºå…³é”®è¯ç»Ÿè®¡
        if enable_analysis:
            print(f"\nğŸ”‘ çƒ­é—¨å…³é”®è¯:")
            top_keywords = repo.get_keyword_statistics(top_n=10)
            for i, item in enumerate(top_keywords[:10], 1):
                print(f"   {i}. {item['keyword']}: {item['count']} æ¬¡")
            
            print(f"\nğŸ“š çƒ­é—¨ä¸»é¢˜:")
            top_topics = repo.get_topic_statistics(top_n=5)
            for i, item in enumerate(top_topics[:5], 1):
                print(f"   {i}. {item['topic']}: {item['count']} æ¬¡")
            
            print(f"\nğŸ“‚ åˆ†ç±»ç»Ÿè®¡:")
            categories = repo.get_category_statistics()
            for category, count in list(categories.items())[:5]:
                print(f"   {category}: {count} ç¯‡")
            
            print(f"\nğŸ˜Š æƒ…æ„Ÿç»Ÿè®¡:")
            sentiments = repo.get_sentiment_statistics()
            for sentiment, count in sentiments.items():
                print(f"   {sentiment}: {count} ç¯‡")
                
    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
    
    # 6. å…³é—­è¿æ¥
    es_client.close()
    print("\nâœ… Elasticsearch è¿æ¥å·²å…³é—­")
    
    return {
        "success": success_count,
        "failed": failed_count,
        "duplicate": duplicate_count,
        "analyzed": analyzed_count,
        "total": success_count + failed_count + duplicate_count
    }


async def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 80)
    print("å¸¦å†…å®¹åˆ†æçš„æ–‡ç« çˆ¬è™«")
    print("=" * 80)
    print()
    
    # é…ç½®é€‰é¡¹
    enable_analysis = input("æ˜¯å¦å¯ç”¨å†…å®¹åˆ†æï¼Ÿ(y/nï¼Œé»˜è®¤ y): ").strip().lower() != 'n'
    check_duplicate = input("æ˜¯å¦å¯ç”¨å»é‡æ£€æµ‹ï¼Ÿ(y/nï¼Œé»˜è®¤ y): ").strip().lower() != 'n'
    
    skip_duplicate = True
    if check_duplicate:
        skip_duplicate = input("æ˜¯å¦è·³è¿‡é‡å¤æ–‡æ¡£ï¼Ÿ(y/nï¼Œé»˜è®¤ y): ").strip().lower() != 'n'
    
    print()
    
    # è¿è¡Œçˆ¬è™«
    result = await scrape_with_content_analysis(
        es_index_name="tophub_articles",
        enable_analysis=enable_analysis,
        check_duplicate=check_duplicate,
        skip_duplicate=skip_duplicate
    )
    
    print("\n" + "=" * 80)
    print("ä»»åŠ¡å®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

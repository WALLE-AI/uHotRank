"""
ä¸»ç¨‹åº - çˆ¬å–æ–‡ç« å¹¶ä¿å­˜åˆ° Elasticsearch
"""
from backend.agent.agent_today_data import scrape_all_articles_to_es


if __name__ == "__main__":
    print("=" * 80)
    print("TopHub æ–‡ç« çˆ¬è™« - è‡ªåŠ¨ä¿å­˜åˆ° Elasticsearch")
    print("=" * 80)
    print()
    
    # è¯¢é—®æ˜¯å¦å¯ç”¨å†…å®¹åˆ†æ
    enable_analysis = input("æ˜¯å¦å¯ç”¨å†…å®¹åˆ†æï¼Ÿ(y/nï¼Œé»˜è®¤ y): ").strip().lower() != 'n'
    print()
    
    # çˆ¬å–æ‰€æœ‰æ–‡ç« å¹¶ä¿å­˜åˆ° ES
    result = scrape_all_articles_to_es(
        es_index_name="tophub_articles",
        batch_size=10,  # æ¯ 10 æ¡æ‰¹é‡æ’å…¥ä¸€æ¬¡
        enable_analysis=enable_analysis  # å¯ç”¨å†…å®¹åˆ†æ
    )
    
    print("\n" + "=" * 80)
    print("ä»»åŠ¡å®Œæˆï¼")
    print("=" * 80)
    print(f"æˆåŠŸ: {result['success']} æ¡")
    print(f"å¤±è´¥: {result['failed']} æ¡")
    if enable_analysis:
        print(f"å·²åˆ†æ: {result['analyzed']} æ¡")
    print(f"æ€»è®¡: {result['total']} æ¡")
    
    print("\nğŸ’¡ æç¤º:")
    print("  - ä½¿ç”¨ search_by_analysis.py æŸ¥çœ‹åˆ†æç»“æœå’Œç»Ÿè®¡")
    print("  - ä½¿ç”¨ test_elasticsearch.py æµ‹è¯•æœç´¢åŠŸèƒ½")
    print("  - ä½¿ç”¨ run_crawler.py é€‰æ‹©ä¸åŒçš„çˆ¬å–æ¨¡å¼")

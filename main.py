# --- ä¸»ç¨‹åº ---
import os
import random
import time
from backend.agent.agent_today_data import MAX_SLEEP, MIN_SLEEP, OUTPUT_FILE, gentle_scrape_content, get_homepage_links, save_to_file, scrape_tophub_dynamic_link


if __name__ == "__main__":
    # 1. è·å–åˆ—è¡¨
    articles_list = scrape_tophub_dynamic_link()
    
    # é™åˆ¶æµ‹è¯•æ•°é‡ (å¦‚æœåªæ˜¯æµ‹è¯•ï¼Œå–æ¶ˆä¸‹é¢è¿™è¡Œçš„æ³¨é‡Š)
    # articles_list = articles_list[:5] 
    
    print(f"\nğŸš€ å¼€å§‹æ¸©å’Œçˆ¬å–ä»»åŠ¡ï¼Œå…± {len(articles_list)} ç¯‡...")
    print(f"ğŸ’¾ æ•°æ®å°†å®æ—¶ä¿å­˜è‡³: {os.path.abspath(OUTPUT_FILE)}\n")

    for i, item in enumerate(articles_list[:5]):
        print(f"[{i+1}/{len(articles_list)}] æ­£åœ¨å¤„ç†: {item['title'][:20]}...")
        
        # 2. æ‰§è¡Œçˆ¬å–
        content_data = gentle_scrape_content(item)
        
        # 3. å®æ—¶ä¿å­˜
        save_to_file(content_data)
        
        # 4. åˆ¤æ–­ç»“æœå¹¶æ‰“å°åé¦ˆ
        if "content" in content_data and len(content_data['content']) > 50:
             print(f"   -> æˆåŠŸ! (æ­£æ–‡çº¦ {len(content_data['content'])} å­—)")
        else:
             print(f"   -> æŠ“å–å†…å®¹è¾ƒå°‘æˆ–å¤±è´¥ (å¯èƒ½éœ€è¦ç™»å½•æˆ–ä¸ºå›¾ç‰‡/è§†é¢‘å†…å®¹)")

        # 5. ã€å…³é”®ã€‘éšæœºæ¸©å’Œç­‰å¾…
        # æ¨¡æ‹Ÿäººç±»é˜…è¯»å®Œä¸€ç¯‡æ–‡ç« åï¼Œå‘å‘†å‡ ç§’å†ç‚¹ä¸‹ä¸€ç¯‡
        sleep_time = random.uniform(MIN_SLEEP, MAX_SLEEP)
        print(f"   -> â˜• ä¼‘æ¯ {sleep_time:.2f} ç§’...\n")
        time.sleep(sleep_time)

    print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæ¯•ï¼")
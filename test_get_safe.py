import time
import random
import csv
import os
import sys
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup

# å¼•å…¥å¼ºå¤§çš„è¯·æ±‚åº“
from curl_cffi import requests as cffi_requests
from playwright.sync_api import sync_playwright

class SafehooStealthSpider:
    def __init__(self):
        self.base_url = "https://www.safehoo.com"
        self.root_url = "https://www.safehoo.com/Build/"
        self.csv_filename = 'safehoo_stealth_data.csv'
        
        # åˆå§‹åŒ– CSV æ–‡ä»¶
        self._init_csv()

    def _init_csv(self):
        if not os.path.exists(self.csv_filename):
            with open(self.csv_filename, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerow(['Category', 'Title', 'Publish_Date', 'Content', 'URL'])

    def get_html_stealth(self, url):
        """
        [èåˆæ ¸å¿ƒ] æ™ºèƒ½è·å–ç½‘é¡µå†…å®¹
        1. ä¼˜å…ˆå°è¯• curl_cffi (é€Ÿåº¦å¿«ï¼Œæ¨¡æ‹Ÿ TLS æŒ‡çº¹)
        2. å¦‚æœå¤±è´¥æˆ–è¢«æ‹¦æˆªï¼Œè‡ªåŠ¨é™çº§åˆ° Playwright (èƒ½åŠ›å¼ºï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨)
        """
        
        # --- ç­–ç•¥ A: é™æ€è¯·æ±‚ (curl_cffi) ---
        # print(f"   [âš¡ é™æ€å°è¯•] {url}") # è°ƒè¯•æ—¶å¯å¼€å¯
        try:
            response = cffi_requests.get(
                url, 
                impersonate="chrome124", 
                headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
                    "Referer": "https://www.safehoo.com/",
                    "Accept-Language": "zh-CN,zh;q=0.9"
                },
                timeout=10
            )
            
            if response.status_code == 200:
                # å…³é”®ï¼šSafehoo æ˜¯è€ç½‘ç«™ï¼Œå¿…é¡»æ‰‹åŠ¨å¤„ç† GBK ç¼–ç 
                # errors='replace' é˜²æ­¢ä¸ªåˆ«ç‰¹æ®Šå­—ç¬¦å¯¼è‡´æŠ¥é”™
                return response.content.decode('utf-8', errors='replace')
            elif response.status_code == 404:
                return None # é¡µé¢çœŸçš„ä¸å­˜åœ¨
            else:
                print(f"   [âš ï¸ é™æ€å¤±è´¥] çŠ¶æ€ç : {response.status_code}ï¼Œå‡†å¤‡åˆ‡æ¢æµè§ˆå™¨...")

        except Exception as e:
            print(f"   [âš ï¸ é™æ€æŠ¥é”™] {e}ï¼Œå‡†å¤‡åˆ‡æ¢æµè§ˆå™¨...")

        # --- ç­–ç•¥ B: åŠ¨æ€æµè§ˆå™¨å…œåº• (Playwright) ---
        print(f"   [ğŸ¢ å¯åŠ¨æµè§ˆå™¨å…œåº•] æ­£åœ¨å¤„ç†: {url} ...")
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(
                    headless=True, 
                    args=['--disable-blink-features=AutomationControlled'] 
                )
                
                context = browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
                )
                
                page = context.new_page()
                
                # è®¿é—®é¡µé¢
                page.goto(url, wait_until="domcontentloaded", timeout=30000)
                
                # [é’ˆå¯¹ Safehoo çš„ä¼˜åŒ–]
                # ç­‰å¾…åˆ—è¡¨é¡µçš„åˆ—è¡¨å®¹å™¨ OR è¯¦æƒ…é¡µçš„æ­£æ–‡å®¹å™¨å‡ºç°
                try:
                    page.wait_for_selector("div.main_list, div.content, td#article_content", timeout=5000)
                except:
                    pass # è¶…æ—¶æœªæ‰¾åˆ°ä¹Ÿä¸æŠ¥é”™ï¼Œç›´æ¥æ‹¿å½“å‰å¿«ç…§

                content = page.content()
                browser.close()
                return content

        except Exception as e:
            print(f"   [âŒ æµè§ˆå™¨ä¹Ÿå¤±è´¥] {e}")
            return None

    def discover_sub_channels(self):
        """é˜¶æ®µä¸€ï¼šä» /Build/ é¦–é¡µæå–æ‰€æœ‰å­ç‰ˆå—"""
        print(f"ğŸ” æ­£åœ¨æ‰«æä¸»é¡µå­ç‰ˆå—: {self.root_url} ...")
        html = self.get_html_stealth(self.root_url)
        if not html:
            print("âŒ æ— æ³•è®¿é—®ä¸»é¡µï¼Œç¨‹åºç»ˆæ­¢ã€‚")
            return []

        soup = BeautifulSoup(html, 'lxml')
        sub_channels = set()

        for a in soup.find_all('a', href=True):
            href = a['href']
            text = a.get_text(strip=True)
            full_url = urljoin(self.root_url, href)
            
            # è¿‡æ»¤é€»è¾‘
            path = urlparse(full_url).path
            if "/Build/" in full_url and full_url != self.root_url:
                if full_url.endswith('/') or "." not in path.split('/')[-1]:
                    if "Index.shtml" not in full_url:
                        sub_channels.add((text, full_url))

        channel_list = list(sub_channels)
        print(f"âœ… å‘ç° {len(channel_list)} ä¸ªå­ç‰ˆå—")
        return channel_list

    def parse_detail(self, url, category_name):
        """é˜¶æ®µä¸‰ï¼šè§£æè¯¦æƒ…é¡µå†…å®¹"""
        html = self.get_html_stealth(url)
        if not html: return None

        soup = BeautifulSoup(html, 'lxml')
        try:
            h1 = soup.find('h1')
            title = h1.get_text(strip=True) if h1 else "æ— æ ‡é¢˜"

            content_div = soup.find('div', {'id': 'content'}) or \
                          soup.find('div', {'class': 'content'}) or \
                          soup.find('td', {'id': 'article_content'})
            
            content = ""
            if content_div:
                for tag in content_div(["script", "style"]):
                    tag.decompose()
                content = content_div.get_text('\n', strip=True)

            info_div = soup.find('div', {'class': 'info'})
            date_info = info_div.get_text(strip=True) if info_div else ""

            return [category_name, title, date_info, content, url]
        except:
            return None

    def crawl_channel(self, name, url):
        """é˜¶æ®µäºŒï¼šéå†å•ä¸ªç‰ˆå—"""
        print(f"\nğŸš€ å¼€å§‹æŠ“å–ç‰ˆå—: [{name}]")
        
        MAX_PAGES = 50 
        
        for page in range(1, MAX_PAGES + 1):
            if page == 1:
                page_url = url.rstrip('/') + "/Index.shtml"
            else:
                page_url = url.rstrip('/') + f"/List_{page}.shtml"
            
            print(f"   ğŸ“‚ [{name}] ç¬¬ {page} é¡µ...")
            html = self.get_html_stealth(page_url)
            
            # 404æ£€æµ‹ï¼šcurl_cffiè¿”å›Noneï¼Œplaywrightå¯èƒ½è¿”å›ç©ºæˆ–é”™è¯¯é¡µ
            if not html or "å¹¶æ²¡æœ‰æ‰¾åˆ°æ‚¨è¦è®¿é—®çš„é¡µé¢" in html:
                print(f"   ğŸ ç‰ˆå— [{name}] ç¿»é¡µç»“æŸ")
                break

            soup = BeautifulSoup(html, 'lxml')
            links = soup.select('div.main_list li a, div.catList li a, ul.list li a')
            
            article_links = []
            for link in links:
                href = link.get('href')
                if href:
                    article_links.append(urljoin(page_url, href))
            
            if not article_links:
                if page == 1: break # ç©ºç‰ˆå—
                else: break # ç¿»é¡µç»“æŸ

            # éå†æ–‡ç« 
            count = 0
            for art_url in article_links:
                data_row = self.parse_detail(art_url, name)
                if data_row:
                    self.save_to_csv(data_row)
                    count += 1
                    sys.stdout.write(f"\r        å·²æŠ“å–: {count}/{len(article_links)}")
                    sys.stdout.flush()
                
                # åªæœ‰åœ¨ä½¿ç”¨ curl_cffi æˆåŠŸæ—¶æ‰éœ€è¦å»¶æ—¶
                # å¦‚æœè§¦å‘äº† Playwrightï¼Œå› ä¸ºå¯åŠ¨æµè§ˆå™¨æœ¬èº«å°±å¾ˆæ…¢ï¼Œå¯ä»¥å‡å°‘ sleep
                time.sleep(random.uniform(0.5, 1.0))
            print("") 

    def save_to_csv(self, row):
        with open(self.csv_filename, 'a', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow(row)

    def run(self):
        channels = self.discover_sub_channels()
        for name, link in channels:
            self.crawl_channel(name, link)
        print("\nğŸ‰ å…¨éƒ¨ä»»åŠ¡å®Œæˆï¼")

if __name__ == "__main__":
    spider = SafehooStealthSpider()
    spider.run()